{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my first deep neural network with pytorch :D\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_hidden1, d_hidden2, d_out):\n",
    "        super().__init__()\n",
    "        #self.n_batch = n_batch\n",
    "        self.d_in = d_in\n",
    "        self.d_hidden1 = d_hidden1\n",
    "        self.d_hidden2 = d_hidden2\n",
    "        self.d_out = d_out\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.d_in, self.d_hidden1)\n",
    "        self.fc2 = nn.Linear(self.d_hidden1, self.d_hidden2)\n",
    "        self.fc3 = nn.Linear(self.d_hidden2, self.d_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "net = Net(784,256,128,10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# defineing a transformation to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# downloading and loading the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "#specifying some variables\n",
    "n_epochs = 30\n",
    "n_batches = 64\n",
    "\n",
    "#defining the loss fn and the optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "#initializing lists to track batch and epoch losses\n",
    "batch_losses = []\n",
    "epoch_losses = []\n",
    "\n",
    "#training of the neural network\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    #initialize running epoch loss to zero\n",
    "    running_epoch_loss = 0\n",
    "    \n",
    "    #reset iterator\n",
    "    dataiter = iter(trainloader)\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        #forwarding iterator to next batch of training images\n",
    "        images, labels = dataiter.next()\n",
    "        \n",
    "        #flattening tensor to an 64x784 tensor\n",
    "        images = images.view(64,-1)    \n",
    "        \n",
    "        #using the Adam optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forword propagation through the network\n",
    "        out = net(images)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "        #tracking batch losses\n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        #update running batch loss\n",
    "        running_epoch_loss += loss\n",
    "        \n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        #updating the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    #tracking epoch losses\n",
    "    epoch_losses.append(running_epoch_loss)\n",
    "\n",
    "PATH = './minst_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2348bb21a48>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#sns.scatterplot(y=batch_losses, x=range(len(batch_losses)))\n",
    "sns.scatterplot(y=epoch_losses, x=range(len(epoch_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n",
      "Accuracy of the network on the 64 test images: 93 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# defineing a transformation to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# downloading and loading the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "n_batches = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in range(n_batches):\n",
    "        images, labels = dataiter.next()\n",
    "        \n",
    "        out = net(images.view(labels.size(0),-1))\n",
    "        ps = torch.exp(out)\n",
    "        \n",
    "        _, predicted = ps.topk(1, dim=1)\n",
    "                \n",
    "        total += labels.size(0)\n",
    "        #correct += (predicted == labels.view(*predicted.shape)).sum().item()\n",
    "        \n",
    "        equals = (predicted == labels.view(*predicted.shape))\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        print(accuracy.item())\n",
    "        \n",
    "print('Accuracy of the network on the %d test images: %d %%' % (n_batches*labels.size(0),\n",
    "    accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # '-'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
